{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0iVuWbfUsMq"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "%pip install -qqq torch torchvision setuptools scikit-learn\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade datasets -qqq accelerate hf-transfer transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vQk2H4cU43r"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Dataset id from huggingface.co/dataset\n",
        "dataset_id = \"burtenshaw/PleIAs_common_corpus_code_classification\"\n",
        "\n",
        "# Load raw dataset\n",
        "dataset = load_dataset(dataset_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scLV2l8DeAXk"
      },
      "outputs": [],
      "source": [
        "print(len(dataset[\"train\"]))\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu03sFAiVBZm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Dataset id from huggingface.co/dataset\n",
        "dataset_id = \"burtenshaw/PleIAs_common_corpus_code_classification\"\n",
        "\n",
        "# Load raw dataset\n",
        "dataset = load_dataset(dataset_id)\n",
        "\n",
        "# Model id to load the tokenizer\n",
        "# model_id = \"answerdotai/ModernBERT-base\"\n",
        "model_id = \"bert-base-cased\"\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Create label mappings (ensure this is done before tokenization if tokenization converts labels)\n",
        "unique_labels = list(set(dataset[\"train\"][\"labels\"])) # Use original dataset to get all labels\n",
        "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
        "\n",
        "\n",
        "# Tokenize helper function\n",
        "def tokenize(batch):\n",
        "    tokenized_inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    # Convert labels from string to integer IDs within the tokenize function\n",
        "    int_labels = []\n",
        "    for label in batch['labels']:\n",
        "         if label in label2id:\n",
        "            int_labels.append(label2id[label])\n",
        "         else:\n",
        "            # Handle cases where a label might be missing (e.g., 'Pickle')\n",
        "            # For now, print a warning and assign a default or skip\n",
        "            print(f\"Warning: Label '{label}' not found in label2id mapping. Assigning -1.\")\n",
        "            int_labels.append(-1) # Assign a default ID, e.g., -1 for unknown\n",
        "            # Or, to skip: continue\n",
        "    tokenized_inputs['labels'] = int_labels # Add integer labels to the tokenized inputs\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Tokenize dataset\n",
        "# Apply tokenization and label conversion in one map step\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"], load_from_cache_file=False)\n",
        "\n",
        "\n",
        "# # Tokenize dataset\n",
        "# tokenized_train_dataset = dataset['train'].select(range(1000)).map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# # Tokenize dataset\n",
        "# tokenized_test_dataset = dataset['test'].select(range(200)).map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(tokenized_dataset[\"train\"].features.keys())\n",
        "# dict_keys(['labels', 'input_ids', 'attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EJxH4YsVBcW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Model id to load the tokenizer\n",
        "# model_id = \"answerdotai/ModernBERT-base\"\n",
        "model_id = \"bert-base-cased\"\n",
        "\n",
        "# Prepare model labels - useful for inference\n",
        "# labels = list(set(tokenized_dataset[\"train\"][\"labels\"]))\n",
        "labels = list(set(tokenized_dataset[\"train\"][\"labels\"]))\n",
        "num_labels = len(labels)\n",
        "# label2id, id2label = dict(), dict()\n",
        "# for i, label in enumerate(labels):\n",
        "#     label2id[label] = str(i)\n",
        "#     id2label[str(i)] = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da-JsWNlg-cO"
      },
      "outputs": [],
      "source": [
        "# Download the model from huggingface.co/models\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb9Ab_ykVBfD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Metric helper method\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    score = f1_score(\n",
        "            labels, predictions, labels=labels, pos_label=1, average=\"weighted\"\n",
        "        )\n",
        "    return {\"f1\": float(score) if score == 1 else score}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOgrfOJJVHSj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"Distillbert-code-classifier\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=5,\n",
        "    bf16=False, # bfloat16 training\n",
        "    optim=\"adamw_torch_fused\", # improved optimizer\n",
        "    # logging & evaluation strategies\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    # push to hub parameters\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_token=HfFolder.get_token(),\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgQsi3tnXJU4"
      },
      "source": [
        "# Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiwCjMY2XJci"
      },
      "outputs": [],
      "source": [
        "limited_dataset = tokenized_dataset[\"train\"].select(range(100))\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=limited_dataset,\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZMjCx_1XiC0"
      },
      "outputs": [],
      "source": [
        "# clear memory\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "del trainer\n",
        "del model\n",
        "del limited_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u86h29rsXJlY"
      },
      "source": [
        "# Underfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOZHoke2XJtk"
      },
      "outputs": [],
      "source": [
        "# define a low learning rate\n",
        "training_args.learning_rate = 1e-7\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=limited_dataset,\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r3jOiQ1Xq-E"
      },
      "outputs": [],
      "source": [
        "# clear memory\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "del trainer\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQRRGOyXKAZ"
      },
      "source": [
        "# Just right! ðŸ¥£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGL6NDFUXKI3"
      },
      "outputs": [],
      "source": [
        "# define a valid learning rate\n",
        "training_args.learning_rate = 5e-5\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=limited_dataset,\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3pV0O3uhc2X"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dSYxaH4VQRY"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# load model from huggingface.co/models using our repository id\n",
        "classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"argilla/ModernBERT-domain-classifier\",\n",
        "    device=0,\n",
        ")\n",
        "\n",
        "sample = \"\"\"def add_numbers(a, b):\n",
        "    return a + b\"\"\"\n",
        "\n",
        "classifier(sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c3f844e"
      },
      "source": [
        "print(tokenized_dataset[\"train\"].features['labels'].dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "599380fc"
      },
      "source": [
        "print(set(tokenized_dataset[\"train\"][\"labels\"]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}